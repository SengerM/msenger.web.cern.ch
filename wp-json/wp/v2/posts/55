{"id":55,"date":"2021-01-08T11:53:44","date_gmt":"2021-01-08T11:53:44","guid":{"rendered":"https:\/\/msenger.web.cern.ch\/?p=55"},"modified":"2021-03-25T08:23:54","modified_gmt":"2021-03-25T08:23:54","slug":"empirical-likelihood-function-with-continuous-parameters","status":"publish","type":"post","link":"https:\/\/msenger.web.cern.ch\/empirical-likelihood-function-with-continuous-parameters\/","title":{"rendered":"Empirical likelihood function with continuous parameters"},"content":{"rendered":"<style>.kb-table-of-content-nav.kb-table-of-content-id_fcf87a-19 .kb-table-of-content-wrap{padding-top:var(--global-kb-spacing-sm, 1.5rem);padding-right:var(--global-kb-spacing-sm, 1.5rem);padding-bottom:var(--global-kb-spacing-sm, 1.5rem);padding-left:var(--global-kb-spacing-sm, 1.5rem);}.kb-table-of-content-nav.kb-table-of-content-id_fcf87a-19 .kb-table-of-contents-title-wrap{padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}.kb-table-of-content-nav.kb-table-of-content-id_fcf87a-19 .kb-table-of-contents-title{font-weight:regular;font-style:normal;}.kb-table-of-content-nav.kb-table-of-content-id_fcf87a-19 .kb-table-of-content-wrap .kb-table-of-content-list{font-weight:regular;font-style:normal;margin-top:var(--global-kb-spacing-sm, 1.5rem);margin-right:0px;margin-bottom:0px;margin-left:0px;}<\/style>\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_7f6535-00, .wp-block-kadence-advancedheading.kt-adv-heading_7f6535-00[data-kb-block=\"kb-adv-heading_7f6535-00\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_7f6535-00 mark, .wp-block-kadence-advancedheading.kt-adv-heading_7f6535-00[data-kb-block=\"kb-adv-heading_7f6535-00\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h2 class=\"kt-adv-heading_7f6535-00 wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_7f6535-00\">Introduction<\/h2>\n\n\n\n<p>Currently I am working with a new type of particle detectors, still in development, called <a rel=\"noreferrer noopener\" href=\"https:\/\/www.google.com\/search?client=ubuntu&amp;hs=uyX&amp;channel=fs&amp;sxsrf=ALeKk00aAHMwLEvEtaHi1A_-af6suRb35A%3A1610022596888&amp;ei=xP72X_zVNemDi-gPmdSu8AE&amp;q=ac-lgad+rsd&amp;oq=ac-lgad+rsd&amp;gs_lcp=CgZwc3ktYWIQAzoECCMQJzoECAAQEzoICAAQDRAeEBM6BggAEA0QHlDEdljbfGCLhgFoAHAAeACAAaoCiAGhCpIBAzItNZgBAKABAaoBB2d3cy13aXrAAQE&amp;sclient=psy-ab&amp;ved=0ahUKEwj8x97H6YnuAhXpwQIHHRmqCx4Q4dUDCAw&amp;uact=5\" data-type=\"URL\" data-id=\"https:\/\/www.google.com\/search?client=ubuntu&amp;hs=uyX&amp;channel=fs&amp;sxsrf=ALeKk00aAHMwLEvEtaHi1A_-af6suRb35A%3A1610022596888&amp;ei=xP72X_zVNemDi-gPmdSu8AE&amp;q=ac-lgad+rsd&amp;oq=ac-lgad+rsd&amp;gs_lcp=CgZwc3ktYWIQAzoECCMQJzoECAAQEzoICAAQDRAeEBM6BggAEA0QHlDEdljbfGCLhgFoAHAAeACAAaoCiAGhCpIBAzItNZgBAKABAaoBB2d3cy13aXrAAQE&amp;sclient=psy-ab&amp;ved=0ahUKEwj8x97H6YnuAhXpwQIHHRmqCx4Q4dUDCAw&amp;uact=5\" target=\"_blank\">AC-LGADs (also RSD)<\/a>. These detectors promise to provide both spacial and temporal measurements for particles hits. Today I am not interested in the details of these detectors or how to use them, but in obtaining a method to infer the hit position of the particle (and also the time in future work) by maximizing the likelihood of the observation.<\/p>\n\n\n\n<p>My current setup is composed by an AC-LGAD detector mounted in a scanning TCT, like <a rel=\"noreferrer noopener\" href=\"http:\/\/particulars.si\/products.php?prod=LargeScanTCT.html\" data-type=\"URL\" data-id=\"http:\/\/particulars.si\/products.php?prod=LargeScanTCT.html\" target=\"_blank\">this one<\/a>, and can be depicted like this:<\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" src=\"https:\/\/msenger.web.cern.ch\/wp-content\/uploads\/2021\/01\/tct-detector-drawing.svg\" alt=\"\" class=\"wp-image-60\" width=\"676\" height=\"271\"\/><\/figure><\/div>\n\n\n\n<p> The detector (light gray square) has 4 readout pads to each of which I connect an oscilloscope. The detector is mounted such that a pulsed laser can be shined on an arbitrary \\( x_0,y_0 \\) position. In this way, I shine the laser at some point and register the signals coming out from the detector. <\/p>\n\n\n\n<p>From these signals I can get different quantities such as the amplitude, the collected charge, etc. So, for example, from pad 1 at some position I have this (you can zoom horizontally to better appreciate the details):<\/p>\n\n\n\n<iframe loading=\"lazy\" src=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/SengerM\/html-github-hosting\/blob\/main\/210107%20empirical%20likelihood%20post\/parsed_signal_example.html\" width=\"100%\" height=\"555\"><\/iframe>\n\n\n\n<p>and something similar for each of the other 3 pads. <\/p>\n\n\n\n<p> Repeating this for many \\( x,y \\) positions I can see how each of these quantities varies as a function of the impact point. So for example the amplitude of the signal seen in pad 1 (channel 1 in the oscilloscope) looks like this: <\/p>\n\n\n\n<iframe loading=\"lazy\" src=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/SengerM\/html-github-hosting\/blob\/main\/210107%20empirical%20likelihood%20post\/amplitude_colormap_example_one_channel.html\" width=\"100%\" height=\"555\"><\/iframe>\n\n\n\n<p><\/p>\n\n\n\n<p> As can be seen, the amplitude is highly dependent with the impact position. There are other quantities such as the collected charge and the time over threshold that also have an interesting dependence on \\(x\\) and \\(y\\). <\/p>\n\n\n\n<p>Since the electron-hole pairs produced by a particle in a silicon detector is an inherently stochastic process, all the signals (and so the amplitude, collected charge, etc) are going to have a random nature with some distribution. So now the question is: How can I use this information to reconstruct the impact position of a particle in this detector?<\/p>\n\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_d5b53d-76, .wp-block-kadence-advancedheading.kt-adv-heading_d5b53d-76[data-kb-block=\"kb-adv-heading_d5b53d-76\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_d5b53d-76 mark, .wp-block-kadence-advancedheading.kt-adv-heading_d5b53d-76[data-kb-block=\"kb-adv-heading_d5b53d-76\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h2 class=\"kt-adv-heading_d5b53d-76 wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_d5b53d-76\">Likelihood maximization<\/h2>\n\n\n\n<p>A very powerful way to estimate quantities when there are random fluctuations involved is to maximize the likelihood function. This is called the <a rel=\"noreferrer noopener\" href=\"https:\/\/en.wikipedia.org\/wiki\/Maximum_likelihood_estimation\" data-type=\"URL\" data-id=\"https:\/\/en.wikipedia.org\/wiki\/Maximum_likelihood_estimation\" target=\"_blank\">maximum likelihood estimator<\/a> and it is extremely general and also has many nice properties.<\/p>\n\n\n\n<p> So, suppose I measure some quantities \\(c_i \\in \\mathcal{C}\\) where \\(i \\in \\{ 1,2,3,4 \\} \\) denotes the pad number and \\( \\mathcal{C}\\) is a set of these quantities that I consider have a nice dependence with the impact point, for example amplitude and\/or collected charge. If I have a single observation of each of the quantities \\( c_i \\), which will I say it was the \\( x \\) and the \\( y \\) coordinates of the particle? Here is where the <em>likelihood function<\/em> comes into play. It is defined as <\/p>\n\n\n\n<div class=\"wp-block-mathml-mathmlblock\">\\[ \\mathscr{L}(c_i | x, y) := \\prod_{C_i \\in \\mathcal{C}} f_{C_i} (c_i | x, y) \\]<script src=\"https:\/\/msenger.web.cern.ch\/wp-includes\/js\/dist\/vendor\/wp-polyfill-inert.min.js?ver=3.1.2\" id=\"wp-polyfill-inert-js\"><\/script>\n<script src=\"https:\/\/msenger.web.cern.ch\/wp-includes\/js\/dist\/vendor\/regenerator-runtime.min.js?ver=0.14.0\" id=\"regenerator-runtime-js\"><\/script>\n<script src=\"https:\/\/msenger.web.cern.ch\/wp-includes\/js\/dist\/vendor\/wp-polyfill.min.js?ver=3.15.0\" id=\"wp-polyfill-js\"><\/script>\n<script src=\"https:\/\/msenger.web.cern.ch\/wp-includes\/js\/dist\/hooks.min.js?ver=c6aec9a8d4e5a5d543a1\" id=\"wp-hooks-js\"><\/script>\n<script src=\"https:\/\/msenger.web.cern.ch\/wp-includes\/js\/dist\/i18n.min.js?ver=7701b0c3857f914212ef\" id=\"wp-i18n-js\"><\/script>\n<script id=\"wp-i18n-js-after\">\nwp.i18n.setLocaleData( { 'text direction\\u0004ltr': [ 'ltr' ] } );\n<\/script>\n<script  async src=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/mathjax\/2.7.7\/MathJax.js?config=TeX-MML-AM_CHTML\" id=\"mathjax-js\"><\/script>\n<\/div>\n\n\n\n<p> with \\( f_{C_i}(c_i | x,y) \\) the probability density function for the quantity \\( C_i \\) for which I have measured the value \\( c_i \\) at point \\( (x,y) \\). Note that \\( x \\) and \\( y \\) are the free variables of this function, because I don&#8217;t know the impact point of the particle. I only know the values of each of the \\( c_i \\)&#8217;s. <\/p>\n<p> According to the <em>principle of maximum likelihood<\/em> my best choice for \\( x \\) and \\( y \\) would be that that maximizes \\( \\mathscr{L} \\). Okay, let&#8217;s do that! <\/p>\n\n\n\n<p> The problem with this is that I don&#8217;t know the (analytic expression for the) functions \\( f_{C_i}(c_i | x,y) \\). To make it worse, \\( x \\) and \\( y \\) are continuous parameters (if they were discrete I could try to measure \\( f_{C_i}(c_i | x,y) \\) at each possible  \\( x \\) and \\( y \\)). So now the problem is how to determine the distribution function for each \\( C_i \\) and its dependence with the impact position. <\/p>\n\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_1882fd-a4, .wp-block-kadence-advancedheading.kt-adv-heading_1882fd-a4[data-kb-block=\"kb-adv-heading_1882fd-a4\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_1882fd-a4 mark, .wp-block-kadence-advancedheading.kt-adv-heading_1882fd-a4[data-kb-block=\"kb-adv-heading_1882fd-a4\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h2 class=\"kt-adv-heading_1882fd-a4 wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_1882fd-a4\">Construction of an experimental likelihood function<\/h2>\n\n\n\n<p>Let&#8217;s see here how to determine experimentally a likelihood function to later using it to determine the impact position of a particle.<\/p>\n\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_191173-ef, .wp-block-kadence-advancedheading.kt-adv-heading_191173-ef[data-kb-block=\"kb-adv-heading_191173-ef\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_191173-ef mark, .wp-block-kadence-advancedheading.kt-adv-heading_191173-ef[data-kb-block=\"kb-adv-heading_191173-ef\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h3 class=\"kt-adv-heading_191173-ef wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_191173-ef\">Measuring a distribution function at a single point<\/h3>\n\n\n\n<p> The first step towards the construction of an experimental likelihood function is to be able to determine a single distribution function \\( f_{C_i}(c_i | x_0,y_0) \\) at some fixed point \\(x_0,y_0\\). It is possible to do this using the <code> <a href=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.gaussian_kde.html\" target=\"_blank\" rel=\"noreferrer noopener\">gaussian_kde<\/a> <\/code> function from the SciPy module. For this the first step is to have samples of the distribution we want to &#8220;measure&#8221; and then it is as simple as giving these samples to the <code>gaussian_kde<\/code> function to obtain an approximation of the probability density function. <\/p>\n\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_bbef72-40, .wp-block-kadence-advancedheading.kt-adv-heading_bbef72-40[data-kb-block=\"kb-adv-heading_bbef72-40\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_bbef72-40 mark, .wp-block-kadence-advancedheading.kt-adv-heading_bbef72-40[data-kb-block=\"kb-adv-heading_bbef72-40\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h5 class=\"kt-adv-heading_bbef72-40 wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_bbef72-40\">Example<\/h5>\n\n\n\n<p>Below there is a <a rel=\"noreferrer noopener\" href=\"https:\/\/en.wikipedia.org\/wiki\/Minimal_working_example\" data-type=\"URL\" data-id=\"https:\/\/en.wikipedia.org\/wiki\/Minimal_working_example\" target=\"_blank\">MWE<\/a> that uses <code>gaussian_kde<\/code> to approximate an arbitrary distribution that was sampled. The samples are artificially generated inside the computer, but in &#8220;real life&#8221; they would be the result of a physical measurement. The code:<\/p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; quick-code: false; notranslate\" title=\"\">\nimport numpy as np\nfrom scipy.stats.kde import gaussian_kde\nimport myplotlib as mpl # https:\/\/github.com\/SengerM\/myplotlib\n\ndef generate_samples(x,y):\n\treturn list(np.random.randn(99999)*(1+x)+y+x) + list((np.random.rand(9999)-.5)*3*(1+x)+y+x)\n\nsamples = generate_samples(5,3)\npdf = gaussian_kde(samples)\n\nfig = mpl.manager.new(\n\ttitle = &#039;Example of gaussian_kde&#039;,\n\txlabel = &#039;Random variable&#039;,\n\tylabel = &#039;Probability density&#039;,\n)\nfig.hist(\n\tsamples,\n\tdensity = True,\n\tlabel = &#039;Measured samples&#039;,\n)\nq = np.linspace(min(samples),max(samples))\nfig.plot(\n\tq,\n\tpdf(q),\n\tlabel = &#039;Gaussian KDE approximation&#039;,\n)\nfig.show()\n<\/pre><\/div>\n\n\n<p>and the output plot looks like this:<\/p>\n\n\n\n<iframe loading=\"lazy\" src=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/SengerM\/html-github-hosting\/blob\/main\/210107%20empirical%20likelihood%20post\/example_of_gaussian_kde.html\" width=\"100%\" height=\"555\"><\/iframe>\n\n\n\n<p>As can be seen we have an approximation of the distribution. Of course this approximation is not perfect and will be different for each realization of the samples (the measured values). But it is better than nothing. And actually looks really good.<\/p>\n\n\n\n<p> Now we can repeat this process for many different \\(x,y\\) points. In the end we will have the distribution \\( f_{C_i}(c_i | x_0,y_0) \\) sampled at many different \\(x_0,y_0\\). <\/p>\n\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_45e329-f0, .wp-block-kadence-advancedheading.kt-adv-heading_45e329-f0[data-kb-block=\"kb-adv-heading_45e329-f0\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_45e329-f0 mark, .wp-block-kadence-advancedheading.kt-adv-heading_45e329-f0[data-kb-block=\"kb-adv-heading_45e329-f0\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h3 class=\"kt-adv-heading_45e329-f0 wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_45e329-f0\">From discrete \\(x,y\\) to continuous \\(x,y\\)<\/h3>\n\n\n\n<p> Once the distribution \\( f_{C_i}(c_i | x,y) \\) was sampled at many \\( x,y \\) points the next step is to somehow interpolate for any intermediate \\( x,y \\). This is not trivial, or at least I have not found any package that solves this in a single line of code. In fact, as we will see, this has not a unique answer and depends on the criterion we want to use. <\/p>\n\n\n\n<p>For this we first need to define some interpolation method, and this requires &#8220;human input&#8221; in order to decide how to do it. Consider for example the following distributions:<\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" src=\"https:\/\/msenger.web.cern.ch\/wp-content\/uploads\/2021\/01\/how_to_interpolate_drawing_1.svg\" alt=\"\" class=\"wp-image-89\" width=\"500\" height=\"330\"\/><\/figure><\/div>\n\n\n\n<p> These can be two measured distributions at two different \\( x \\) values (let&#8217;s ignore \\( y \\) for simplicity). Now let&#8217;s think of how the distribution should look like for an intermediate \\( x \\) : <\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" src=\"https:\/\/msenger.web.cern.ch\/wp-content\/uploads\/2021\/01\/how_to_interpolate_drawing_2.svg\" alt=\"\" class=\"wp-image-90\" width=\"500\" height=\"330\"\/><\/figure><\/div>\n\n\n\n<p>As can be seen there are many different options, and depending on what we want to do any can be the correct answer. In my current case, however, the green one seems to be the correct answer. I.e. the distribution should somehow move and transform smoothly from one to another instead of &#8220;disappearing here and appearing there&#8221; as in the violet drawing. Even after deciding this, there are probably many different ways (which yield slightly different results) of implementing this. Below I will share my procedure.<\/p>\n\n\n\n<p> To perform the interpolation between the different discrete \\( x,y\\) points I proceeded in two steps: \n<\/p><ol>\n    <li>Interpolate the mean values \\( \\mu_0 \\) and \\( \\mu_1 \\) to obtain a new \\( \\mu \\). In 1D this is very easy: Just interpolate using a straight line between the two points. In 2D (as in my case) it is not as trivial as using a plane, but fortunately the Python community has already solved this problem. Of course there is more than one approach here, but let&#8217;s keep it simple and use an easy and reasonable solution.<\/li>\n    <li>Interpolate the profile of the density function. By this I mean the shape of the function itself. Again, here there are many different options on how to do this. I will discuss it late onr.<\/li>\n<\/ol>\n<p>The following drawings should help in visualizing this process. In the first drawing I show a straight-line-interpolation for the mean value \\( \\mu \\) of the interpolated distribution: <\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" src=\"https:\/\/msenger.web.cern.ch\/wp-content\/uploads\/2021\/01\/how_to_interpolate_drawing_3.svg\" alt=\"\" class=\"wp-image-97\" width=\"610\" height=\"430\"\/><\/figure><\/div>\n\n\n\n<p>In the second drawing (below) I show one possibility for interpolating the profile of the distribution. As can be seen the two distributions were shifted such that they share the same mean value and in this condition a weighted average is performed. Details will come later.<\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" src=\"https:\/\/msenger.web.cern.ch\/wp-content\/uploads\/2021\/01\/how_to_interpolate_drawing_4-1.svg\" alt=\"\" class=\"wp-image-121\" width=\"610\" height=\"320\"\/><\/figure><\/div>\n\n\n\n<p> These drawings show the case for a unique parameter \\( x \\). For a two dimensional parameters space \\( x,y \\) it is the same idea. First I performed the interpolation of the mean value and then the interpolation of the function profile. This time, however, there are 4 distributions to &#8220;mix&#8221; in order to obtain the interpolated distribution (I assumed a rectangular grid of sampled points). For the interpolation of the mean value I used the <code><a href=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.interpolate.interp2d.html\" data-type=\"URL\" data-id=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.interpolate.interp2d.html\" target=\"_blank\" rel=\"noreferrer noopener\">scipy.interpolate.interp2d<\/a><\/code> function which performs a interpolation using splines. So: <\/p>\n\n\n\n<div class=\"wp-block-mathml-mathmlblock\">\\[ \\mu=\\text{scipy.interpolate.interp2d}\\left(\\left[\\begin{matrix}x_{0} &amp; x_{0} &amp; x_{1} &amp; x_{1}\\end{matrix}\\right],\\left[\\begin{matrix}y_{0} &amp; y_{1} &amp; y_{0} &amp; y_{1}\\end{matrix}\\right],\\left[\\begin{matrix}\\mu_{00} &amp; \\mu_{01} &amp; \\mu_{10} &amp; \\mu_{11}\\end{matrix}\\right]\\right) (x,y) \\]<\/div>\n\n\n\n<p>where each of the sub indices denotes which of the four points is being considered:<\/p>\n\n\n\n<div class=\"wp-block-image\"><figure class=\"aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" src=\"https:\/\/msenger.web.cern.ch\/wp-content\/uploads\/2021\/01\/rectangular_grid_for_interpolation.svg\" alt=\"\" class=\"wp-image-101\" width=\"401\" height=\"245\"\/><\/figure><\/div>\n\n\n\n<p> For the interpolation of the function profile I proceeded manually. First I shifted the functions such that their mean value was \\( \\mu \\) and then I performed an average weighted by the area of the &#8220;opposite rectangle&#8221;: <\/p>\n\n\n\n<div class=\"wp-block-mathml-mathmlblock\">\\[ f\\left(q|x,y\\right)=\\sum_{i\\in\\left\\{ 0,1\\right\\} }\\sum_{j\\in\\left\\{ 0,1\\right\\} }f_{ij}\\left(q+\\mu_{ij}-\\mu\\right)\\frac{\\left|x-x_{1-i}\\right|\\left|y-y_{1-j}\\right|}{\\left(x_{1}-x_{0}\\right)\\left(y_{1}-y_{0}\\right)} \\]<\/div>\n\n\n\n<p> where \\(f_{ij} \\) is the measured distribution at point \\( x_i,y_j \\) and \\( q \\equiv c_i \\) is just a relabeling. <\/p>\n\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_88d109-5a, .wp-block-kadence-advancedheading.kt-adv-heading_88d109-5a[data-kb-block=\"kb-adv-heading_88d109-5a\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_88d109-5a mark, .wp-block-kadence-advancedheading.kt-adv-heading_88d109-5a[data-kb-block=\"kb-adv-heading_88d109-5a\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h5 class=\"kt-adv-heading_88d109-5a wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_88d109-5a\">Example<\/h5>\n\n\n\n<p>The results of applying these formulae look pretty good, here is an example:<\/p>\n\n\n\n<iframe loading=\"lazy\" src=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/SengerM\/html-github-hosting\/blob\/main\/210107%20empirical%20likelihood%20post\/interpolation_example.html\" width=\"100%\" height=\"555\"><\/iframe>\n\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_995d5d-82, .wp-block-kadence-advancedheading.kt-adv-heading_995d5d-82[data-kb-block=\"kb-adv-heading_995d5d-82\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_995d5d-82 mark, .wp-block-kadence-advancedheading.kt-adv-heading_995d5d-82[data-kb-block=\"kb-adv-heading_995d5d-82\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h3 class=\"kt-adv-heading_995d5d-82 wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_995d5d-82\">Finally, the empirical likelihood<\/h3>\n\n\n\n<p> Combining all the previous stuff I am finally able to construct the &#8220;empirical likelihood&#8221; in order to use it for parameter estimation. From here on it is just a problem about how to implement this in a computer. Fortunately, although not trivial, this is not so hard with to do in Python. The code below simulates a fake data set that represents some measured quantity at different \\( x,y \\) positions, then it builds approximates the distribution at each point using KDE estimation, after this it produces an interpolation to get continuous parameters, and finally it builds the likelihood function and maximizes it: <\/p>\n\n\n<div class=\"wp-block-syntaxhighlighter-code \"><pre class=\"brush: python; title: ; quick-code: false; notranslate\" title=\"\">\nimport numpy as np\nfrom scipy.stats.kde import gaussian_kde\nfrom scipy import interpolate\n\ndef check_number_or_numpy_array(var, name: str):\n\tif not (isinstance(var,int) or isinstance(var,float) or isinstance(var, np.ndarray)):\n\t\traise TypeError(f&#039;&lt;{name}&gt; must be a number (int or float) or a numpy array, received instead &lt;{name}&gt; of type {type(var)}.&#039;)\n\nclass ExperimentalParametricDensity:\n\tdef __init__(self, samples: dict):\n\t\t# &lt;samples&gt; is a dict that contains the data in each xy sampled point, for example samples&#91;x]&#91;y] = samples_array_at_x_y.\n\t\t# Assuming that the x,y points are distributed in a rectangular mesh, i.e. it is just a matrix.\n\t\tself._samples = samples\n\t\tself._x_vals = sorted(&#91;x for x in samples])\n\t\tself._y_vals = sorted(&#91;y for y in samples&#91;self._x_vals&#91;0]]]) # Here I am using that the x,y points are distributed in a rectangular mesh.\n\t\t\n\t@property\n\tdef means(self):\n\t\tif not hasattr(self, &#039;_means&#039;):\n\t\t\tself._means = np.zeros((len(self._y_vals), len(self._x_vals)))\n\t\t\tself._means&#91;:] = float(&#039;NaN&#039;)\n\t\t\tfor nx,x in enumerate(self._x_vals):\n\t\t\t\tfor ny,y in enumerate(self._y_vals):\n\t\t\t\t\tself._means&#91;ny,nx] = np.nanmean(self._samples&#91;x]&#91;y])\n\t\treturn self._means\n\t\n\t@property\n\tdef xx(self):\n\t\tif not hasattr(self, &#039;_xx&#039;):\n\t\t\txx,yy = np.meshgrid(self._x_vals, self._y_vals)\n\t\t\tself._xx = xx\n\t\t\tself._yy = yy\n\t\treturn self._xx\n\t\n\t@property\n\tdef yy(self):\n\t\tif not hasattr(self, &#039;_yy&#039;):\n\t\t\tself.xx\n\t\treturn self._yy\n\t\n\t@property\n\tdef kde(self):\n\t\t# Returns a dictionary with the KDE at each sampled x,y. This means that returns the same as self._samples but instead of having samples it has the KDE functions. Each KDE function has the signature KDE(q).\n\t\tif not hasattr(self, &#039;_kde&#039;):\n\t\t\tself._kde = {}\n\t\t\tfor x in self._x_vals:\n\t\t\t\tself._kde&#91;x] = {}\n\t\t\t\tfor y in self._y_vals:\n\t\t\t\t\tself._kde&#91;x]&#91;y] = gaussian_kde(self._samples&#91;x]&#91;y])\n\t\treturn self._kde\n\t\n\tdef __call__(self, q, x, y):\n\t\tif not (isinstance(x,int) or isinstance(x,float)):\n\t\t\traise TypeError(f&#039;&lt;x&gt; must be a number (int or float), received instead &lt;x&gt; of type {type(x)}.&#039;)\n\t\tif not (isinstance(y,int) or isinstance(y,float)):\n\t\t\traise TypeError(f&#039;&lt;y&gt; must be a number (int or float), received instead &lt;y&gt; of type {type(y)}.&#039;)\n\t\tif not min(self._x_vals) &lt;= x &lt;= max(self._x_vals):\n\t\t\traise ValueError(f&#039;&lt;x&gt; must be bounded within the sampling region. The minimum value for &lt;x&gt; is {min(self._x_vals)} and the maximum {max(self._x_vals)}, received x = {x}.&#039;)\n\t\tif not min(self._y_vals) &lt;= y &lt;= max(self._y_vals):\n\t\t\traise ValueError(f&#039;&lt;y&gt; must be bounded within the sampling region. The minimum value for &lt;y&gt; is {min(self._y_vals)} and the maximum {max(self._y_vals)}, received y = {y}.&#039;)\n\t\tcheck_number_or_numpy_array(q, &#039;q&#039;)\n\t\t# If we are here is because x,y are both numbers and within the sampling range.\n\t\tif x == self._x_vals&#91;0]: idx_prev_x = 0\n\t\telse: idx_prev_x = np.where((np.array(self._x_vals)&lt;x))&#91;0]&#91;-1]\n\t\tif y == self._y_vals&#91;0]: idx_prev_y = 0\n\t\telse: idx_prev_y = np.where((np.array(self._y_vals)&lt;y))&#91;0]&#91;-1]\n\t\tx0 = self._x_vals&#91;idx_prev_x]\n\t\ty0 = self._y_vals&#91;idx_prev_y]\n\t\tx1 = self._x_vals&#91;idx_prev_x+1]\n\t\ty1 = self._y_vals&#91;idx_prev_y+1]\n\t\t\u00b500 = self.means&#91;idx_prev_y,idx_prev_x]\n\t\t\u00b501 = self.means&#91;idx_prev_y+1,idx_prev_x]\n\t\t\u00b510 = self.means&#91;idx_prev_y,idx_prev_x+1]\n\t\t\u00b511 = self.means&#91;idx_prev_y+1,idx_prev_x+1]\n\t\tf00 = self.kde&#91;x0]&#91;y0]\n\t\tf01 = self.kde&#91;x0]&#91;y1]\n\t\tf10 = self.kde&#91;x1]&#91;y0]\n\t\tf11 = self.kde&#91;x1]&#91;y1]\n\t\t\u00b5 = interpolate.interp2d(x=&#91;x0,x0,x1,x1], y=&#91;y0,y1,y0,y1], z=&#91;\u00b500,\u00b501,\u00b510,\u00b511])(x,y)\n\t\treturn (f00(q+\u00b500-\u00b5)*(x1-x)*(y1-y) + f01(q+\u00b501-\u00b5)*(x1-x)*(y-y0) + f10(q+\u00b510-\u00b5)*(x-x0)*(y1-y) + f11(q+\u00b511-\u00b5)*(x-x0)*(y-y0))\/(x1-x0)\/(y1-y0)\n\t\n\tdef likelihood(self, x, y, q):\n\t\tif not (isinstance(q,int) or isinstance(q,float)):\n\t\t\traise TypeError(f&#039;&lt;q&gt; must be a number (int or float), received instead &lt;q&gt; of type {type(q)}.&#039;)\n\t\tcheck_number_or_numpy_array(x, &#039;x&#039;)\n\t\tcheck_number_or_numpy_array(y, &#039;y&#039;)\n\t\tif isinstance(x, float) or isinstance(x, int):\n\t\t\tx = np.array(&#91;x])\n\t\tif isinstance(y, float) or isinstance(y, int):\n\t\t\ty = np.array(&#91;y])\n\t\tif x.shape != y.shape:\n\t\t\traise ValueError(f&#039;The shape of &lt;x&gt; and &lt;y&gt; must be the same. Received x.shape = {x.shape} and y.shape = {y.shape}.&#039;)\n\t\tll = &#91;]\n\t\tfor xx,yy in zip(x.ravel(),y.ravel()):\n\t\t\tll.append(self(q,xx,yy))\n\t\tll = np.reshape(np.array(ll),x.shape)\n\t\tif x.shape == 1:\n\t\t\tll = ll&#91;0]\n\t\treturn ll\n\t\t\n\nif __name__ == &#039;__main__&#039;:\n\timport myplotlib as mpl # https:\/\/github.com\/SengerM\/myplotlib\n\timport lmfit\n\timport palettable\n\t\n\tdef generate_samples(x,y):\n\t\treturn list(np.random.randn(99999)*(1+x)+y+x) + list((np.random.rand(9999)-.5)*3*(1+x)+y+x)\n\n\tX_VALS = &#91;0,1,2,3]\n\tY_VALS = &#91;0,1,2,3]\n\n\tdata = {}\n\tqmin = 0\n\tqmax = 0\n\tfor x in X_VALS:\n\t\tdata&#91;x] = {}\n\t\tfor y in Y_VALS:\n\t\t\tdata&#91;x]&#91;y] = generate_samples(x,y)\n\t\t\tqmin = min(qmin, min(data&#91;x]&#91;y]))\n\t\t\tqmax = max(qmax, max(data&#91;x]&#91;y]))\n\n\tepd = ExperimentalParametricDensity(data)\n\t\n\tfig = mpl.manager.new(\n\t\ttitle = &#039;Measured and estimated distributions at each x,y&#039;,\n\t\txlabel = &#039;q&#039;,\n\t\tylabel = &#039;Probability density&#039;,\n\t)\n\tqaxis = np.linspace(qmin, qmax,99)\n\tx0 = 1.1\n\ty0 = 2.6\n\tn = 0\n\tfor x in X_VALS:\n\t\tfor y in Y_VALS:\n\t\t\tn += 1\n\t\t\tcolor = tuple(np.array(palettable.tableau.Tableau_20.colors&#91;n%20])\/255)\n\t\t\tfig.hist(\n\t\t\t\tdata&#91;x]&#91;y],\n\t\t\t\tlabel = f&#039;Measured q at x={x}, y={y}&#039;,\n\t\t\t\tdensity = True,\n\t\t\t\tcolor = color\n\t\t\t)\n\t\t\tfig.plot(\n\t\t\t\tqaxis,\n\t\t\t\tepd.kde&#91;x]&#91;y](qaxis),\n\t\t\t\tlabel = f&#039;KDE at x={x}, y={y}&#039;,\n\t\t\t\tcolor = color,\n\t\t\t\tlinestyle = &#039;--&#039;,\n\t\t\t)\n\tfig.plot(\n\t\tqaxis,\n\t\tepd(qaxis, x0, y0),\n\t\tlabel = f&#039;Interpolation example at x={x0}, y={y0}&#039;,\n\t)\n\n\tq0 = 6\n\txaxis = np.linspace(min(X_VALS), max(X_VALS))\n\tyaxis = np.linspace(min(Y_VALS), max(Y_VALS))\n\txx,yy = np.meshgrid(xaxis,yaxis)\n\tfig = mpl.manager.new(\n\t\ttitle = f&#039;Likelihood plot&#039;,\n\t\txlabel = &#039;x&#039;,\n\t\tylabel = &#039;y&#039;,\n\t\taspect = &#039;equal&#039;,\n\t)\n\tfig.contour(\n\t\tx = xx,\n\t\ty = yy,\n\t\tz = epd.likelihood(xx, yy, q0),\n\t\tcolorscalelabel = f&#039;Likelihood(q={q0}|x,y)&#039;,\n\t)\n\t\n\t# Now let&#039;s find the maximum of the likelihood:\n\tparams = lmfit.Parameters()\n\tparams.add(&#039;x&#039;, value = 0, min = min(X_VALS), max = max(X_VALS))\n\tparams.add(&#039;y&#039;, value = 0, min = min(Y_VALS), max = max(Y_VALS))\n\tdef func2minimize(pars):\n\t\tparvals = pars.valuesdict()\n\t\treturn 1\/epd.likelihood(parvals&#91;&#039;x&#039;],parvals&#91;&#039;y&#039;],q0)\n\tminimizer = lmfit.Minimizer(\n\t\tfunc2minimize,\n\t\tparams,\n\t)\n\tminimizer_result = minimizer.minimize(method = &#039;nelder&#039;)\n\tprint(minimizer_result.params)\n\t\n\tfig.plot(\n\t\t&#91;minimizer_result.params&#91;&#039;x&#039;].value],\n\t\t&#91;minimizer_result.params&#91;&#039;y&#039;].value],\n\t\tmarker = &#039;.&#039;,\n\t)\n\n\tmpl.manager.show()\n\n<\/pre><\/div>\n\n\n<p>As a result of running this script we obtain the following plots:<\/p>\n\n\n\n<p>First this plot that shows all the &#8220;measured&#8221; (simulated) distributions as histograms along with the corresponding KDE estimation for this distribution (you can enable\/hide traces by clicking in the legend):<\/p>\n\n\n\n<iframe loading=\"lazy\" src=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/SengerM\/html-github-hosting\/blob\/main\/210107%20empirical%20likelihood%20post\/final_example_1.html\" width=\"100%\" height=\"555\"><\/iframe>\n\n\n\n<div class=\"wp-block-columns alignfull is-layout-flex wp-container-core-columns-layout-1 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\" style=\"flex-basis:100%\"><\/div>\n<\/div>\n\n\n\n<p> and secondly this color map with the likelihood function for the particular realization \\( q = q_0 = 6 \\) :<\/p>\n\n\n\n<iframe loading=\"lazy\" src=\"https:\/\/htmlpreview.github.io\/?https:\/\/github.com\/SengerM\/html-github-hosting\/blob\/main\/210107%20empirical%20likelihood%20post\/final_example_2.html\" width=\"100%\" height=\"555\"><\/iframe>\n\n\n\n<p> The black points are the \\( x,y \\) points where the distribution of \\( q \\) was measured (simulated). The red point is the maximum of the likelihood function, this means that \\( x_\\text{red},y_\\text{red} \\) is the estimated point when \\( q = q_0 = 6 \\) according to the <em>maximum likelihood principle<\/em>. <\/p>\n\n\n\n<p>Of course it is possible to use this likelihood function for whatever we want, for example to construct other estimators such as the <a rel=\"noreferrer noopener\" href=\"https:\/\/en.wikipedia.org\/wiki\/Likelihood-ratio_test\" data-type=\"URL\" data-id=\"https:\/\/en.wikipedia.org\/wiki\/Likelihood-ratio_test\" target=\"_blank\">likelihood ratio test<\/a>.<\/p>\n\n\n<style>.wp-block-kadence-advancedheading.kt-adv-heading_2f2a18-34, .wp-block-kadence-advancedheading.kt-adv-heading_2f2a18-34[data-kb-block=\"kb-adv-heading_2f2a18-34\"]{font-style:normal;}.wp-block-kadence-advancedheading.kt-adv-heading_2f2a18-34 mark, .wp-block-kadence-advancedheading.kt-adv-heading_2f2a18-34[data-kb-block=\"kb-adv-heading_2f2a18-34\"] mark{font-style:normal;color:#f76a0c;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px;}<\/style>\n<h2 class=\"kt-adv-heading_2f2a18-34 wp-block-kadence-advancedheading\" data-kb-block=\"kb-adv-heading_2f2a18-34\">Conclusion<\/h2>\n\n\n\n<p>I presented a way to obtain an estimation of the likelihood function, i.e. an experimental likelihood function, for continuous parameters based only in measurements. This method employs a sampling of the distribution function in discrete points for the parameters and then performs a specific interpolation to go from discrete to continuous values.<\/p>\n\n\n\n<p>My <a href=\"https:\/\/msenger.web.cern.ch\/first-application-of-the-empirical-likelihood-function-to-position-reconstruction-in-ac-lgad-detectors\/#the-position-reconstruction-algorithm\" data-type=\"URL\" data-id=\"https:\/\/msenger.web.cern.ch\/first-application-of-the-empirical-likelihood-function-to-position-reconstruction-in-ac-lgad-detectors\/#the-position-reconstruction-algorithm\">next post<\/a> will show the application of this method to real data coming out of the detectors.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>Introduction Currently I am working with a new type of particle detectors, still in development, called AC-LGADs (also RSD). These detectors promise to provide both spacial and temporal measurements for particles hits. Today I am not interested in the details of these detectors or how to use them, but in obtaining a method to infer &hellip;<\/p>\n<p class=\"read-more\"> <a class=\"\" href=\"https:\/\/msenger.web.cern.ch\/empirical-likelihood-function-with-continuous-parameters\/\"> <span class=\"screen-reader-text\">Empirical likelihood function with continuous parameters<\/span> Read More &raquo;<\/a><\/p>\n","protected":false},"author":1,"featured_media":145,"comment_status":"closed","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"inline_featured_image":false,"footnotes":""},"categories":[1],"tags":[24],"_links":{"self":[{"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/posts\/55"}],"collection":[{"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/comments?post=55"}],"version-history":[{"count":74,"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/posts\/55\/revisions"}],"predecessor-version":[{"id":689,"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/posts\/55\/revisions\/689"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/media\/145"}],"wp:attachment":[{"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/media?parent=55"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/categories?post=55"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/msenger.web.cern.ch\/wp-json\/wp\/v2\/tags?post=55"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}